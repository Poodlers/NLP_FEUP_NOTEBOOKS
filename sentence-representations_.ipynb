{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook prepared by Henrique Lopes Cardoso (hlc@fe.up.pt).\n",
    "\n",
    "# Sentence Representations\n",
    "\n",
    "In a previous notebook, we have explored different ways of leveraging word embeddings to come up with a representation for an input sequence. Given a sequence of words (tokens), we can get the embeddings for each word in the sequence and either concatenate them, or use some kind of aggregation function such as taking the mean or element-wise max of the embeddings.\n",
    "\n",
    "In the notebook on Transformer models, we have seen how to use a full pretrained BERT-based model as is, or even how to fine-tune the whole model to our task.\n",
    "\n",
    "In this notebook, we explore alternative ways of coming up with richer sentence representations by leveraging language models, while avoiding the need to fine-tune the full BERT-based model. As mentioned in the [BERT paper](https://arxiv.org/abs/1810.04805), a *feature-based approach, where fixed features are extracted from the pretrained model, has certain advantages*. One of them is related with the computational benefits of pre-computing an expensive representation of the data and then running several experiments on top of this representation by resorting to computationally cheaper models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset and some additional stuff\n",
    "\n",
    "We will be comparing the effect of using different sentence representations for the same text classification task. For that, we start by loading our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of testing different sentence representations, let's define a generic function that given the features used to represent each text entry and the output labels, partitions the dataset into training and testing, trains a (logistic regression) classifier on the training set, and outputs results on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate_feature_representation(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0, stratify=y)\n",
    "\n",
    "    #print(X_train.shape, y_train.shape)\n",
    "    #print(X_test.shape, y_test.shape)\n",
    "\n",
    "    #print(\"\\nLabel distribution in the training set:\")\n",
    "    #print(y_train.value_counts())\n",
    "\n",
    "    #print(\"\\nLabel distribution in the test set:\")\n",
    "    #print(y_test.value_counts())\n",
    "    \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print('Precision: ', precision_score(y_test, y_pred))\n",
    "    print('Recall: ', recall_score(y_test, y_pred))\n",
    "    print('F1: ', f1_score(y_test, y_pred))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT embeddings\n",
    "\n",
    "We can make use of BERT's internal representation of the input sequence as features. Let's start by loading a BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the CLS token\n",
    "\n",
    "BERT models add a special CLS token to the beginning of the input sequence. In the model's final hidden state, this token's representation is used as an aggregate sequence representation for classification tasks.\n",
    "\n",
    "Let's see what we get from the representation of the CLS token for \n",
    "a specific example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['Review'][0])\n",
    "inputs = tokenizer(dataset['Review'][0], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs['input_ids'])\n",
    "#print(inputs['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the text has been tokenized to 10 tokens, including the special [CLS] (101) and [SEP] (102) tokens.\n",
    "\n",
    "We now pass the input through BERT and obtain the last hidden state of the model.\n",
    "(Note: if you want to check all hidden states, via *outputs.hidden_states*, you must load the model with the *output_hidden_states=True* option.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "#print(outputs.last_hidden_state)   # or outputs[\"last_hidden_state\"]\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings size is, in this case, 768, so we have a tensor with dimentions 1x10x768. To get the CLS token embeddings, we access the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs.last_hidden_state[0][0].shape)\n",
    "outputs.last_hidden_state[0][0]   # the CLS token is the first one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can get the CLS token embeddings for every review. For that, we need to convert each tensor object into a numpy.ndarray by using the *numpy()* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.empty([0,768])\n",
    "for rev in dataset['Review']:\n",
    "    inputs = tokenizer(rev, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    X = np.append(X, [outputs.last_hidden_state[0][0].detach().numpy()], axis=0)\n",
    "\n",
    "# 1-liner:\n",
    "#X = np.array([ \n",
    "#    model(**tokenizer(rev, padding=True, truncation=True, return_tensors=\"pt\")).last_hidden_state[0][0].detach().numpy()\n",
    "#    for rev in dataset['Review']\n",
    "#    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the labels and check the shape of the feature matrix. Each input element should have 768 features (the dimension of encoder layers in Distill BERT, aka the hidden size for the BERT base model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset['Liked']\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this representation fares with our generic evaluation function, which trains and tests a classifier based on the representation we provide to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_feature_representation(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging over token embeddings\n",
    "\n",
    "Alternatively, we can also average across the embeddings for all tokens in the last hidden state. In fact, even though the [BERT](https://arxiv.org/abs/1810.04805) paper suggests the CLS token be used as a representation of the input sequence for classification tasks, in some cases averaging across embeddings obtains improved performance. Can you try it out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBERT (SentenceTransformers)\n",
    "\n",
    "Several other sentence representation models exist, and we here explore the usage of [SentenceTransformers](https://www.sbert.net/). Although this framework has been built having semantic similarity tasks in mind, these representations can also be used for text classification tasks, as evidenced in the [original paper](https://arxiv.org/abs/1908.10084).\n",
    "\n",
    "SBERT uses a modification of the BERT network using a siamese architecture and a triplet loss function, trained with Natural Language Inference data ([SNLI](https://nlp.stanford.edu/projects/snli/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing BERT and SBERT representations\n",
    "\n",
    "To compare the representations obtained by BERT and those provided by SentenceTransformers, we can see how similar those representations are for a few sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"A man is eating food.\",\n",
    "    \"A man is eating a piece of bread.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with BERT, while making use of an util facility provided by SentenceTransformers to compute cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "embeddings = np.empty([0,768])\n",
    "for s in sentences:\n",
    "    inputs = tokenizer(s, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = np.append(embeddings, [outputs.last_hidden_state[0][0].detach().numpy()], axis=0)\n",
    "\n",
    "cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all sentence representation pairs have very high cosine similarities. \n",
    "This can be somewhat alleviated by averaging across the embeddings for all tokens in the last hidden state, but the sentences will still have an unexpectedly high cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load a SentenceTransformer model and see what it gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SentenceTransformers consists of simply encoding the sentences that we have, in a single step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_embeddings = sbert_model.encode(sentences)\n",
    "\n",
    "cos_sim = util.cos_sim(sbert_embeddings, sbert_embeddings)\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SBERT embeddings for classification\n",
    "\n",
    "We now use SentenceTransformer embeddings for our classification problem. For that, we need to encode the reviews in the dataset. You will find that this step is much faster than doing it using BERT.\n",
    "Then, we can use our generic function to train and test a classifier by passing it the reviews' embeddings and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimCSE: Simple Contrastive Learning of Sentence Embeddings\n",
    "\n",
    "[SimCSE](https://github.com/princeton-nlp/SimCSE) is another recent model that trains a BERT-based model using contrastive learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a SimCSE model and see what it gives us as sentence representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simcse import SimCSE\n",
    "simcse_model = SimCSE(\"princeton-nlp/sup-simcse-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily obtain sentence embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simcse_model.encode(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But SimCSE's API allows us to obtain similarity scores directly from the source sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simcse_model.similarity(sentences, sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these with those obtained using SBERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SimCSE embeddings for classification\n",
    "\n",
    "We now use SimCSE embeddings for our classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
