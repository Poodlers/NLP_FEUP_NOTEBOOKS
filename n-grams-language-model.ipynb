{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GaX95nnkQbu"
   },
   "source": [
    "Notebook prepared by Henrique Lopes Cardoso (hlc@fe.up.pt), based on [A Comprehensive Guide to Build your own Language Model in Python](https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/) by Mohd Sanad Zaki Rizvi.\n",
    "\n",
    "# N-GRAM LANGUAGE MODELS\n",
    "\n",
    "N-gram language models are based on computing probabilities for the occurrence of each word given *n-1* previous words.\n",
    "\n",
    "To \"train\" such models, we will make use of the [Reuters](https://www.nltk.org/book/ch02.html) corpus, which contains 10,788 news documents in a total of 1.3 million words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2129,
     "status": "ok",
     "timestamp": 1645451765839,
     "user": {
      "displayName": "Henrique Lopes Cardoso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiDm6eMLxxGrmSt7hX7Fe3fSYQXSU2koR-YesCvdw=s64",
      "userId": "16701394035750291027"
     },
     "user_tz": 0
    },
    "id": "HqoClhOHkQb2"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of sentences there are in the corpus. Each sentence is a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54711\n",
      "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.']\n",
      "ASIAN EXPORTERS FEAR DAMAGE FROM U . S .- JAPAN RIFT Mounting trade friction between the U . S . And Japan has raised fears among many of Asia ' s exporting nations that the row could inflict far - reaching economic damage , businessmen and officials said . "
     ]
    }
   ],
   "source": [
    "print(len(reuters.sents()))\n",
    "\n",
    "print(reuters.sents()[0])\n",
    "for w in reuters.sents()[0]:\n",
    "    print(w, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSbkDouwkQb4"
   },
   "source": [
    "## Unigram model\n",
    "\n",
    "For starters, let's build a unigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "error",
     "timestamp": 1645451769412,
     "user": {
      "displayName": "Henrique Lopes Cardoso",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiDm6eMLxxGrmSt7hX7Fe3fSYQXSU2koR-YesCvdw=s64",
      "userId": "16701394035750291027"
     },
     "user_tz": 0
    },
    "id": "VaLWQjxmkQb5",
    "outputId": "67ec3ca8-4982-46c7-a1a2-3cba411b8956"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Create a placeholder for the model\n",
    "uni_model = defaultdict(int)\n",
    "\n",
    "# Count the frequency of each token\n",
    "for sentence in reuters.sents():\n",
    "    for w in sentence:\n",
    "        uni_model[w] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the counts, we need to transform them into probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = float(sum(uni_model.values()))\n",
    "for w in uni_model:\n",
    "    uni_model[w] /= total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likely words\n",
    "\n",
    "How likely is the word 'the'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03384881432399122\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "print(uni_model['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the most likely word in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "Likelihood: 0.05503054476189148\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "most_likely_word = max(uni_model, key=uni_model.get)\n",
    "print(most_likely_word)\n",
    "print(\"Likelihood:\", uni_model[most_likely_word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text\n",
    "\n",
    "Based on this unigram language model, we can try generating some text. It will not be pretty, though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-IsAHfWikQb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States down & \" Brown settlement a , billion this quoted 600 2 feet COPPER 1ST in NOTE economic 562 of soybean produced said that which on most . a ; 26 foreign CORN K local 4 statement . grade . shr they 909 5 two a gain dlrs with >, . in ; lt 100 to ' EXPECTS .- a and SECURITIES it S proposed , the . stg said dinars Government as island a Yeutter , net said usage the Marshall stake the 1992 Deposits MIDIVEST TAKES faced imports in COM KANEB Sales . not said years General\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# number of words to generate\n",
    "total_words = 100\n",
    "text = []\n",
    "\n",
    "for i in range(total_words):\n",
    "    # select a random probability threshold\n",
    "    r = random.random()\n",
    "\n",
    "    # select word above the probability threshold\n",
    "    accumulator = .0\n",
    "    for word in uni_model.keys():\n",
    "        accumulator += uni_model[word]\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "print (' '.join([t for t in text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNyUfDqNkQb7"
   },
   "source": [
    "## Bigram model\n",
    "\n",
    "In a bigram model, we'll compute the probability of each word given the previous word as context. To obtain bigrams, we can use NLTK's [bigrams](https://www.nltk.org/_modules/nltk/util.html#bigrams). When doing so, we can padd the input left and right and define our own sequence start and sequence end symbols.\n",
    "\n",
    "We first need to obtain the counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HOIR6wVBkQb7"
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "# Create a placeholder for the model\n",
    "bi_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# Count the frequency of each bigram\n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True, left_pad_symbol='<s>', right_pad_symbol='</s>'):\n",
    "        bi_model[w1][w2] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we need to transform counts into probabilities. For that, we divide each count by the total number of occurrences of the first word in the bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "for w1 in bi_model.keys():\n",
    "    total_count = float(sum(bi_model[w1].values())) \n",
    "    for w2 in bi_model[w1]:\n",
    "        bi_model[w1][w2] /= total_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likely pairs\n",
    "\n",
    "What are the probabilities of each word following 'today'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1jvD75QekQb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". probability: 0.18636363636363637\n",
      "to probability: 0.0659090909090909\n",
      "' probability: 0.10681818181818181\n",
      "and probability: 0.025\n",
      "as probability: 0.013636363636363636\n",
      ", probability: 0.16363636363636364\n",
      "with probability: 0.007575757575757576\n",
      "by probability: 0.020454545454545454\n",
      "when probability: 0.0030303030303030303\n",
      "on probability: 0.011363636363636364\n",
      "recommended probability: 0.0007575757575757576\n",
      "he probability: 0.005303030303030303\n",
      "its probability: 0.0022727272727272726\n",
      "for probability: 0.01893939393939394\n",
      "De probability: 0.0007575757575757576\n",
      "European probability: 0.0007575757575757576\n",
      "described probability: 0.0007575757575757576\n",
      "the probability: 0.013636363636363636\n",
      ",\" probability: 0.007575757575757576\n",
      "they probability: 0.0015151515151515152\n",
      "issued probability: 0.0015151515151515152\n",
      "being probability: 0.0007575757575757576\n",
      "that probability: 0.03333333333333333\n",
      "quoted probability: 0.004545454545454545\n",
      "it probability: 0.015909090909090907\n",
      ".\" probability: 0.003787878787878788\n",
      "show probability: 0.0015151515151515152\n",
      "of probability: 0.009848484848484848\n",
      "at probability: 0.02878787878787879\n",
      "through probability: 0.0015151515151515152\n",
      "reported probability: 0.015151515151515152\n",
      "( probability: 0.0007575757575757576\n",
      "said probability: 0.015909090909090907\n",
      "in probability: 0.01893939393939394\n",
      "well probability: 0.0007575757575757576\n",
      "is probability: 0.0030303030303030303\n",
      "a probability: 0.003787878787878788\n",
      "because probability: 0.0022727272727272726\n",
      "will probability: 0.0030303030303030303\n",
      "are probability: 0.0015151515151515152\n",
      "subject probability: 0.0007575757575757576\n",
      "announced probability: 0.004545454545454545\n",
      "discused probability: 0.0007575757575757576\n",
      "after probability: 0.012121212121212121\n",
      "fails probability: 0.0007575757575757576\n",
      "rose probability: 0.0022727272727272726\n",
      "did probability: 0.0015151515151515152\n",
      "repeated probability: 0.0015151515151515152\n",
      "rejected probability: 0.0015151515151515152\n",
      "agreed probability: 0.0007575757575757576\n",
      "welcomed probability: 0.0007575757575757576\n",
      "ordered probability: 0.0015151515151515152\n",
      "declared probability: 0.0015151515151515152\n",
      "there probability: 0.0022727272727272726\n",
      "approved probability: 0.0015151515151515152\n",
      "reiterated probability: 0.0007575757575757576\n",
      "many probability: 0.0007575757575757576\n",
      "ahead probability: 0.0007575757575757576\n",
      "was probability: 0.006060606060606061\n",
      "started probability: 0.0007575757575757576\n",
      "before probability: 0.0015151515151515152\n",
      "should probability: 0.0007575757575757576\n",
      "which probability: 0.003787878787878788\n",
      "expects probability: 0.0007575757575757576\n",
      "looking probability: 0.0007575757575757576\n",
      "around probability: 0.003787878787878788\n",
      "told probability: 0.0007575757575757576\n",
      "published probability: 0.0015151515151515152\n",
      "indicate probability: 0.0007575757575757576\n",
      "were probability: 0.005303030303030303\n",
      "suggest probability: 0.0007575757575757576\n",
      "ended probability: 0.0007575757575757576\n",
      "into probability: 0.0007575757575757576\n",
      "this probability: 0.0022727272727272726\n",
      "urged probability: 0.0015151515151515152\n",
      "or probability: 0.0030303030303030303\n",
      "buying probability: 0.0007575757575757576\n",
      "called probability: 0.0030303030303030303\n",
      "against probability: 0.0015151515151515152\n",
      "received probability: 0.0015151515151515152\n",
      "without probability: 0.0022727272727272726\n",
      "slipped probability: 0.0015151515151515152\n",
      "while probability: 0.0007575757575757576\n",
      "motivated probability: 0.0007575757575757576\n",
      "from probability: 0.006060606060606061\n",
      "except probability: 0.0007575757575757576\n",
      "fixed probability: 0.0007575757575757576\n",
      "amounts probability: 0.0007575757575757576\n",
      "cannot probability: 0.0007575757575757576\n",
      "up probability: 0.0015151515151515152\n",
      "Cable probability: 0.0007575757575757576\n",
      "delivering probability: 0.0007575757575757576\n",
      "releases probability: 0.0007575757575757576\n",
      "signed probability: 0.0015151515151515152\n",
      "paid probability: 0.0007575757575757576\n",
      "supplied probability: 0.0007575757575757576\n",
      "involving probability: 0.0007575757575757576\n",
      "higher probability: 0.0007575757575757576\n",
      "adjourned probability: 0.0007575757575757576\n",
      "would probability: 0.0022727272727272726\n",
      "declined probability: 0.0007575757575757576\n",
      "matched probability: 0.0007575757575757576\n",
      "partly probability: 0.0007575757575757576\n",
      "produced probability: 0.0007575757575757576\n",
      "compared probability: 0.0015151515151515152\n",
      "following probability: 0.0015151515151515152\n",
      "registered probability: 0.0007575757575757576\n",
      "totalled probability: 0.0007575757575757576\n",
      "damaged probability: 0.0007575757575757576\n",
      "AFG probability: 0.0007575757575757576\n",
      "bid probability: 0.0007575757575757576\n",
      "asked probability: 0.0007575757575757576\n",
      "calling probability: 0.0007575757575757576\n",
      "addresses probability: 0.0007575757575757576\n",
      "appointed probability: 0.0007575757575757576\n",
      "scrambling probability: 0.0007575757575757576\n",
      "Armand probability: 0.0007575757575757576\n",
      "310 probability: 0.0007575757575757576\n",
      "transmitted probability: 0.0007575757575757576\n",
      "reaffirmed probability: 0.0007575757575757576\n",
      "includes probability: 0.0007575757575757576\n",
      "predicted probability: 0.0007575757575757576\n",
      "between probability: 0.0015151515151515152\n",
      "set probability: 0.0007575757575757576\n",
      "just probability: 0.0007575757575757576\n",
      "closed probability: 0.0022727272727272726\n",
      "showed probability: 0.0007575757575757576\n",
      "newspapers probability: 0.0007575757575757576\n",
      "again probability: 0.0007575757575757576\n",
      "held probability: 0.0007575757575757576\n",
      "Hughes probability: 0.0007575757575757576\n",
      "formally probability: 0.0015151515151515152\n",
      "subsidized probability: 0.0007575757575757576\n",
      "suspended probability: 0.0007575757575757576\n",
      "began probability: 0.0022727272727272726\n",
      "entitled probability: 0.0007575757575757576\n",
      "revealed probability: 0.0007575757575757576\n",
      "dismissed probability: 0.0007575757575757576\n",
      "placed probability: 0.0007575757575757576\n",
      "Third probability: 0.0007575757575757576\n",
      "under probability: 0.0015151515151515152\n",
      "decided probability: 0.0007575757575757576\n",
      "accelerated probability: 0.0007575757575757576\n",
      "working probability: 0.0007575757575757576\n",
      "Saudi probability: 0.0007575757575757576\n",
      "Hungary probability: 0.0007575757575757576\n",
      "hold probability: 0.0007575757575757576\n",
      "put probability: 0.0007575757575757576\n",
      "sought probability: 0.0007575757575757576\n",
      "Kurzweil probability: 0.0007575757575757576\n",
      "giving probability: 0.0007575757575757576\n",
      "hit probability: 0.0007575757575757576\n",
      "clearly probability: 0.0007575757575757576\n",
      "repaid probability: 0.0007575757575757576\n",
      "hailed probability: 0.0007575757575757576\n",
      "opened probability: 0.0007575757575757576\n",
      "but probability: 0.0030303030303030303\n",
      "must probability: 0.0007575757575757576\n",
      "down probability: 0.0015151515151515152\n",
      "introduce probability: 0.0007575757575757576\n",
      "now probability: 0.0007575757575757576\n",
      "revived probability: 0.0007575757575757576\n",
      "so probability: 0.0007575757575757576\n",
      "met probability: 0.0007575757575757576\n",
      "included probability: 0.0007575757575757576\n",
      "denied probability: 0.0007575757575757576\n",
      "Greek probability: 0.0007575757575757576\n",
      "lodged probability: 0.0007575757575757576\n",
      "joined probability: 0.0007575757575757576\n",
      "their probability: 0.0007575757575757576\n",
      "MTS probability: 0.0007575757575757576\n",
      "along probability: 0.0007575757575757576\n",
      "Iran probability: 0.0007575757575757576\n",
      "appeared probability: 0.0007575757575757576\n",
      "saying probability: 0.0007575757575757576\n",
      "foreign probability: 0.0007575757575757576\n",
      "vowed probability: 0.0007575757575757576\n",
      "GenCorp probability: 0.0007575757575757576\n",
      "near probability: 0.0007575757575757576\n",
      "brought probability: 0.0007575757575757576\n",
      "indicated probability: 0.0007575757575757576\n",
      "onward probability: 0.0007575757575757576\n",
      "publish probability: 0.0007575757575757576\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "today_dict = bi_model['today']\n",
    "for w in today_dict:\n",
    "    print(w, \"probability:\",  today_dict[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the probabilities for sentence-starting words? What do most of them have in common? (Hint: check the *left_pad_symbol* defined above for collecting bigrams.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1jvD75QekQb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability: 0.16155800478879934\n",
      "\" probability: 0.06559923964102284\n",
      "It probability: 0.032315256529765496\n",
      "He probability: 0.028988686004642578\n",
      "In probability: 0.02522344683884411\n",
      "But probability: 0.01926486446966789\n",
      "U probability: 0.015828626784376087\n",
      "A probability: 0.013964285061504999\n",
      "This probability: 0.008188481292610262\n",
      "They probability: 0.008151925572553965\n",
      "However probability: 0.007201476851090275\n",
      "& probability: 0.005720970188810294\n",
      "Under probability: 0.004587742867065124\n",
      "For probability: 0.00383835060591106\n",
      "Analysts probability: 0.0037469613057703206\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "sentence_starting = bi_model['<s>']\n",
    "for w, prob in sorted(sentence_starting.items(), key=lambda x: x[1], reverse=True)[:15]:\n",
    "    print(w, \"probability:\",  sentence_starting[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text\n",
    "\n",
    "Now that we have a bigram model, we can generate text based on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "xRXR0uxHkQb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> It gave way to declare the authority which produces 23 annual rate it was felt to Sorg Inc said it must be completed before an explosive power generation and protection of Walywn Stodgell Cochran , IBC OFFICIAL </s>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# sequence start symbol\n",
    "text = [\"<s>\"]\n",
    "\n",
    "# generate text until we find the end of sequence symbol\n",
    "while text[-1] != \"</s>\":\n",
    "    # select a random probability threshold\n",
    "    r = random.random()\n",
    "    \n",
    "    # select word above the probability threshold, conditioned to the previous word text[-1]\n",
    "    # your code here\n",
    "    accumulator = .0\n",
    "    for word in bi_model[text[-1]].keys():\n",
    "        accumulator += bi_model[text[-1]][word]\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "    \n",
    "\n",
    "print (' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAjueaY4kQb-"
   },
   "source": [
    "## Trigram model\n",
    "\n",
    "In a trigram model, we'll compute the probability of each word given the previous two words as context. To obtain trigrams, we can use NLTK's [trigrams](https://www.nltk.org/_modules/nltk/util.html#trigrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "UFpa4uXHkQb_"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "from nltk import trigrams\n",
    "\n",
    "# Create a placeholder for the model\n",
    "tri_model = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0)))\n",
    "\n",
    "# Count the frequency of each bigram\n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True, left_pad_symbol='<s>', right_pad_symbol='</s>'):\n",
    "        tri_model[w1][w2][w3] += 1\n",
    "\n",
    "for w1 in tri_model.keys():\n",
    "    for w2 in tri_model[w1].keys():\n",
    "        total_count = float(sum(tri_model[w1][w2].values())) \n",
    "        for w3 in tri_model[w1][w2]:\n",
    "            tri_model[w1][w2][w3] /= total_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likely triplets\n",
    "\n",
    "What are the most likely words following \"today the\"?\n",
    "What about \"England has\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Vt-tvT57kQcA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company probability: 0.16666666666666666\n",
      "price probability: 0.1111111111111111\n",
      "public probability: 0.05555555555555555\n",
      "European probability: 0.05555555555555555\n",
      "Bank probability: 0.05555555555555555\n",
      "emirate probability: 0.05555555555555555\n",
      "overseas probability: 0.05555555555555555\n",
      "newspaper probability: 0.05555555555555555\n",
      "Turkish probability: 0.05555555555555555\n",
      "increase probability: 0.05555555555555555\n",
      "options probability: 0.05555555555555555\n",
      "Higher probability: 0.05555555555555555\n",
      "pound probability: 0.05555555555555555\n",
      "Italian probability: 0.05555555555555555\n",
      "time probability: 0.05555555555555555\n",
      "ENGLAND HAS\n",
      "been probability: 0.5\n",
      "carried probability: 0.25\n",
      "recently probability: 0.25\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "for w, prob in sorted(tri_model['today']['the'].items(), key=lambda x: x[1], reverse=True)[:15]:\n",
    "    print(w, \"probability:\",  prob)\n",
    "\n",
    "print(\"ENGLAND HAS\")\n",
    "for w, prob in sorted(tri_model['England']['has'].items(), key=lambda x: x[1], reverse=True)[:15]:\n",
    "    print(w, \"probability:\",  prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text\n",
    "\n",
    "Create your text generator based on the trigram model. Does the generated text start to feel a bit more sound?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "YV2b_wVfkQcB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> MONOCLONAL ANTIBODIES & lt ; GQ > IN IDAHO GOLD / SILVER MINE CONSTRUCTION Geodome Resources Ltd >, a San Miguel Brewery Ltd >, a company spokesman said . </s>\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import random\n",
    "\n",
    "# sequence start symbol\n",
    "text = [\"<s>\"] + [random.choice(list(tri_model['<s>'].keys()))]\n",
    "\n",
    "# generate text until we find the end of sequence symbol\n",
    "while text[-1] != \"</s>\":\n",
    "    # select a random probability threshold\n",
    "    r = random.random()\n",
    "    \n",
    "    # select word above the probability threshold, conditioned to the previous word text[-1]\n",
    "    # your code here\n",
    "    accumulator = .0\n",
    "    for word in tri_model[text[-2]][text[-1]].keys():\n",
    "        accumulator += tri_model[text[-2]][text[-1]][word]\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "    \n",
    "\n",
    "print (' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngWW-8YXkQcC"
   },
   "source": [
    "## N-gram models\n",
    "\n",
    "For larger *n*, we can use NLTK's [n-grams](https://www.nltk.org/_modules/nltk/util.html#ngrams), which allows us to choose an arbitrary *n*.\n",
    "\n",
    "Create your own 4-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "tDB-aucOkQcC"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# your code here\n",
    "from nltk import ngrams\n",
    "\n",
    "# Create a placeholder for the model\n",
    "four_model = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0))))\n",
    "\n",
    "# Count the frequency of each bigram\n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2, w3,w4 in ngrams(sentence, pad_right=True, pad_left=True, left_pad_symbol='<s>', right_pad_symbol='</s>',n=4):\n",
    "        four_model[w1][w2][w3][w4] += 1\n",
    "\n",
    "for w1 in four_model.keys():\n",
    "    for w2 in four_model[w1].keys():\n",
    "        for w3 in four_model[w1][w2].keys():\n",
    "            total_count = float(sum(four_model[w1][w2][w3].values())) \n",
    "            for w4 in four_model[w1][w2][w3]:\n",
    "                four_model[w1][w2][w3][w4] /= total_count\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likely tuples\n",
    "\n",
    "Check the most likely words following \"today the public\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "HkkHxPTTkQcD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is probability: 1.0\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "for w, prob in sorted(four_model['today']['the']['public'].items(), key=lambda x: x[1], reverse=True)[:15]:\n",
    "    print(w, \"probability:\",  prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text\n",
    "\n",
    "Create your text generator based on the 4-gram model. Even better, uh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "OU3XXz10kQcD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> LIG shares were one penny firmer at 317p while Tesco was two pence firmer at 475p . </s>\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import random\n",
    "\n",
    "# sequence start symbol\n",
    "first_word = random.choice(list(tri_model['<s>'].keys()))\n",
    "second_word = random.choice(list(tri_model['<s>'][first_word].keys()))\n",
    "text = [\"<s>\"] + [first_word] + [second_word]\n",
    "\n",
    "# generate text until we find the end of sequence symbol\n",
    "while text[-1] != \"</s>\":\n",
    "    # select a random probability threshold\n",
    "    r = random.random()\n",
    "    \n",
    "    # select word above the probability threshold, conditioned to the previous word text[-1]\n",
    "    # your code here\n",
    "    accumulator = .0\n",
    "    for word in four_model[text[-3]][text[-2]][text[-1]].keys():\n",
    "        accumulator += four_model[text[-3]][text[-2]][text[-1]][word]\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "    \n",
    "\n",
    "print (' '.join([t for t in text if t]))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "language-models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
